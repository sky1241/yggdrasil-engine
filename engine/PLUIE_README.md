# ğŸŒ§ï¸ Phase 4 â€” PLUIE LOCALE

Co-occurrence matrix from OpenAlex full snapshot (~478M papers).

## What it does

Streams all OpenAlex `.gz` files, extracts concept pairs present in Yggdrasil's
5,459 symbols, and builds a sparse co-occurrence matrix. This matrix IS the rain
of Yggdrasil â€” the real connections between scientific domains measured across
all published research.

## Scripts

| Script | Purpose |
|--------|---------|
| `engine/build_cooccurrence.py` | Main pipeline â€” streams .gz, builds sparse matrix |
| `engine/analyze_pluie.py` | Post-analysis â€” stats, degrees, strates, structural holes |
| `tests/generate_mock_data.py` | Mock data generator for testing |
| `tests/test_pluie_bulletproof.py` | 63 tests â€” unit, integration, edge cases, stress |

## Usage

```bash
# 1. Test with mock data
python tests/generate_mock_data.py
YGG_WORKS_DIR="/tmp/mock_openalex/works" python engine/build_cooccurrence.py --test 5
python engine/analyze_pluie.py

# 2. Real run (Windows, ~6-12h for 400GB)
python engine/build_cooccurrence.py

# 3. Interrupted? Resume from checkpoint
python engine/build_cooccurrence.py --resume

# 4. Analyze results
python engine/analyze_pluie.py
```

## Environment Variables

| Variable | Default | Purpose |
|----------|---------|---------|
| `YGG_WORKS_DIR` | `D:\openalex\data\works` | Path to OpenAlex .gz files |
| `YGG_STRATES` | `data\strates_export_v2.json` | Yggdrasil symbols export |
| `YGG_OA_MAP` | `data\openalex_map.json` | Symbol â†’ OpenAlex ID mapping |
| `YGG_OUTPUT` | `data\pluie` | Output directory for matrix + index |

## Output Files

```
data/pluie/
â”œâ”€â”€ cooccurrence_matrix.npz   # scipy sparse CSR matrix (NÃ—N)
â”œâ”€â”€ matrix_index.json         # idxâ†”concept mapping + stats
â”œâ”€â”€ structural_holes.json     # top 200 under-connected pairs (PREDICTIONS)
â”œâ”€â”€ _checkpoint.json          # resume state (cleaned after completion)
â””â”€â”€ _partial_matrix.npz       # partial matrix (cleaned after completion)
```

## Key Design Decisions

- **Streaming**: never loads a full .gz in RAM â€” line by line with `gzip.open()`
- **Sparse accumulation**: `defaultdict(int)` for building, `scipy.sparse.csr_matrix` for storage
- **Checkpoint every 50 files**: Ctrl+C triggers clean save, `--resume` picks up
- **Concept score threshold**: 0.3 (configurable via `--min-score`)
- **Symmetric matrix**: `matrix[i,j] == matrix[j,i]`, diagonal = paper count per concept
- **Structural holes**: pairs where `observed / expected < 0.1` = future discovery candidates

## Tests

```bash
python -m pytest tests/test_pluie_bulletproof.py -v
```

63 tests covering:
- **LoadConcepts** (10): formats, duplicates, unicode, empty, dict values, fallbacks
- **StreamPapers** (8): normal, empty lines, malformed JSON, corrupted .gz, unicode, huge lines
- **ExtractConcepts** (11): score thresholds, missing fields, URL formats, fallbacks
- **DiscoverFiles** (6): flat, nested, deeply nested, sorting, non-gz filtering
- **Checkpoint** (3): save/load/overwrite
- **FullPipeline** (11): 3-concept exact counts, symmetry, multi-gz, score threshold, exit conditions
- **AnalyzePluie** (1): load + stats + degrees
- **EdgeCases** (7): boundary scores, duplicate concepts, whitespace IDs, 10k co-occurrences
- **MathCoherence** (3): symmetry, non-negative, diagonal invariant
- **Stress** (1): 500 concepts Ã— 10k papers < 30s

Bugs found by tests:
- `zlib.error` not caught on corrupted .gz â†’ fixed
- Diagonal counting only when â‰¥2 concepts â†’ confirmed correct behavior

## Validated

Mock test (31 concepts, 2,500 papers): âœ… pipeline, âœ… analysis, âœ… strate clustering
Stress test (500 concepts, 10,000 papers): âœ… 2.9s, symmetric, non-negative
Full test suite: 63/63 âœ…
